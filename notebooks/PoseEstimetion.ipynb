{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## download mpii dataset"
   ],
   "metadata": {
    "id": "nFrkAACOEQlh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -c https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz && \\\n",
    "mkdir -p /content/data/mpii && \\\n",
    "tar -xzf mpii_human_pose_v1.tar.gz -C /content/data/mpii/ && \\\n",
    "rm /content/mpii_human_pose_v1.tar.gz && \\\n",
    "wget -c https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip && \\\n",
    "unzip -qq -j -o mpii_human_pose_v1_u12_2.zip -d /content/data/mpii/annotations && \\\n",
    "rm /content/mpii_human_pose_v1_u12_2.zip"
   ],
   "metadata": {
    "id": "dq2NtdwXmhx4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "def download_from_colab(file_id: str, output_file: str):\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, output_file, quiet=True)"
   ],
   "metadata": {
    "id": "sTrP6MFFoPPO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "download_from_colab(\n",
    "    file_id=\"1DZm9_erQd9EbHASYzpaZsjFIKQSkHLi4\",\n",
    "    output_file=\"/content/data/mpii/annotations/mpii_gt_val.mat\",\n",
    ")\n",
    "\n",
    "download_from_colab(\n",
    "    file_id=\"1goXOVSr-ne8_ZaH80KzEbyAPGn67bs5N\",\n",
    "    output_file=\"/content/data/mpii/annotations/mpii_train.json\",\n",
    ")\n",
    "\n",
    "download_from_colab(\n",
    "    file_id=\"1B5laHYDFN8oShENI958nTVobU6jeHgi2\",\n",
    "    output_file=\"/content/data/mpii/annotations/mpii_val.json\",\n",
    ")"
   ],
   "metadata": {
    "id": "bBjt_s8IoSRQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## download test video"
   ],
   "metadata": {
    "id": "kcnRzl8sM4XW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p /content/data/\n",
    "!wget -O /content/data/video.mp4 https://download.openmmlab.com/mmpose/v1/projects/just_dance/tom.mp4"
   ],
   "metadata": {
    "id": "xDlGyu2CM4F4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check files"
   ],
   "metadata": {
    "id": "x30PU2iVqpem"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!ls /content/data/mpii/images/033441445.jpg"
   ],
   "metadata": {
    "id": "fEsfIShtCOmd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!head -c 250 /content/data/mpii/annotations/mpii_val.json"
   ],
   "metadata": {
    "id": "9NqO6RO3CObR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## install deps"
   ],
   "metadata": {
    "id": "i5Y26VvYbd0N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9KpLHEqqRFi"
   },
   "outputs": [],
   "source": [
    "%pip install -qq -U openmim\n",
    "!mim install -qq mmengine\n",
    "!mim install -qq \"mmcv>=2.0.0\"\n",
    "!mim install -qq \"mmdet>=3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install -qq git+https://github.com/jin-s13/xtcocoapi"
   ],
   "metadata": {
    "id": "iKa4xZzG9DVx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6asGFghrX-L"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/open-mmlab/mmpose.git\n",
    "%cd /content/mmpose\n",
    "%pip install -qq -r requirements.txt\n",
    "%pip install -qq -v -e .\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install -qq decord"
   ],
   "metadata": {
    "id": "mxBf_L0SLWTK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check installations"
   ],
   "metadata": {
    "id": "hmZxtdkcDiTB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# check NVCC version\n",
    "!nvcc -V\n",
    "\n",
    "# check GCC version\n",
    "!gcc --version\n",
    "\n",
    "# check python in conda environment\n",
    "!which python"
   ],
   "metadata": {
    "id": "tKuLGcOlpoLt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check Pytorch installation\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "print(\"torch version:\", torch.__version__, torch.cuda.is_available())\n",
    "print(\"torchvision version:\", torchvision.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiler_version, get_compiling_cuda_version\n",
    "\n",
    "\n",
    "print(\"cuda version:\", get_compiling_cuda_version())\n",
    "print(\"compiler information:\", get_compiler_version())\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmpose\n",
    "\n",
    "\n",
    "print(\"mmpose version:\", mmpose.__version__)"
   ],
   "metadata": {
    "id": "Zo-ZdgtU9R5D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## run validation using `test.py`"
   ],
   "metadata": {
    "id": "1H6CEET6sQgX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p /content/models && \\\n",
    "mim download mmpose --config td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256 --dest /content/models/"
   ],
   "metadata": {
    "id": "qH2eGxBYsYIm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile /content/models/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256-custom.py\n",
    "auto_scale_lr = dict(base_batch_size=512)\n",
    "backend_args = dict(backend='local')\n",
    "codec = dict(\n",
    "    heatmap_size=(\n",
    "        64,\n",
    "        64,\n",
    "    ),\n",
    "    input_size=(\n",
    "        256,\n",
    "        256,\n",
    "    ),\n",
    "    sigma=2,\n",
    "    type='MSRAHeatmap',\n",
    "    unbiased=True)\n",
    "custom_hooks = [\n",
    "    dict(type='SyncBuffersHook'),\n",
    "]\n",
    "data_mode = 'topdown'\n",
    "data_root = 'data/mpii/'\n",
    "dataset_type = 'MpiiDataset'\n",
    "default_hooks = dict(\n",
    "    badcase=dict(\n",
    "        badcase_thr=5,\n",
    "        enable=False,\n",
    "        metric_type='loss',\n",
    "        out_dir='badcase',\n",
    "        type='BadCaseAnalysisHook'),\n",
    "    checkpoint=dict(\n",
    "        interval=10, rule='greater', save_best='PCK', type='CheckpointHook'),\n",
    "    logger=dict(interval=50, type='LoggerHook'),\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "    visualization=dict(enable=False, type='PoseVisualizationHook'))\n",
    "default_scope = 'mmpose'\n",
    "env_cfg = dict(\n",
    "    cudnn_benchmark=False,\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
    "load_from = None\n",
    "log_level = 'INFO'\n",
    "log_processor = dict(\n",
    "    by_epoch=True, num_digits=6, type='LogProcessor', window_size=50)\n",
    "model = dict(\n",
    "    backbone=dict(\n",
    "        extra=dict(\n",
    "            stage1=dict(\n",
    "                block='BOTTLENECK',\n",
    "                num_blocks=(4, ),\n",
    "                num_branches=1,\n",
    "                num_channels=(64, ),\n",
    "                num_modules=1),\n",
    "            stage2=dict(\n",
    "                block='BASIC',\n",
    "                num_blocks=(\n",
    "                    4,\n",
    "                    4,\n",
    "                ),\n",
    "                num_branches=2,\n",
    "                num_channels=(\n",
    "                    48,\n",
    "                    96,\n",
    "                ),\n",
    "                num_modules=1),\n",
    "            stage3=dict(\n",
    "                block='BASIC',\n",
    "                num_blocks=(\n",
    "                    4,\n",
    "                    4,\n",
    "                    4,\n",
    "                ),\n",
    "                num_branches=3,\n",
    "                num_channels=(\n",
    "                    48,\n",
    "                    96,\n",
    "                    192,\n",
    "                ),\n",
    "                num_modules=4),\n",
    "            stage4=dict(\n",
    "                block='BASIC',\n",
    "                num_blocks=(\n",
    "                    4,\n",
    "                    4,\n",
    "                    4,\n",
    "                    4,\n",
    "                ),\n",
    "                num_branches=4,\n",
    "                num_channels=(\n",
    "                    48,\n",
    "                    96,\n",
    "                    192,\n",
    "                    384,\n",
    "                ),\n",
    "                num_modules=3)),\n",
    "        in_channels=3,\n",
    "        init_cfg=dict(\n",
    "            checkpoint=\n",
    "            'https://download.openmmlab.com/mmpose/pretrain_models/hrnet_w48-8ef0771d.pth',\n",
    "            type='Pretrained'),\n",
    "        type='HRNet'),\n",
    "    data_preprocessor=dict(\n",
    "        bgr_to_rgb=True,\n",
    "        mean=[\n",
    "            123.675,\n",
    "            116.28,\n",
    "            103.53,\n",
    "        ],\n",
    "        std=[\n",
    "            58.395,\n",
    "            57.12,\n",
    "            57.375,\n",
    "        ],\n",
    "        type='PoseDataPreprocessor'),\n",
    "    head=dict(\n",
    "        decoder=dict(\n",
    "            heatmap_size=(\n",
    "                64,\n",
    "                64,\n",
    "            ),\n",
    "            input_size=(\n",
    "                256,\n",
    "                256,\n",
    "            ),\n",
    "            sigma=2,\n",
    "            type='MSRAHeatmap',\n",
    "            unbiased=True),\n",
    "        deconv_out_channels=None,\n",
    "        in_channels=48,\n",
    "        loss=dict(type='KeypointMSELoss', use_target_weight=True),\n",
    "        out_channels=16,\n",
    "        type='HeatmapHead'),\n",
    "    test_cfg=dict(flip_mode='heatmap', flip_test=True, shift_heatmap=True),\n",
    "    type='TopdownPoseEstimator')\n",
    "optim_wrapper = dict(optimizer=dict(lr=0.0005, type='Adam'))\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
    "    dict(\n",
    "        begin=0,\n",
    "        by_epoch=True,\n",
    "        end=210,\n",
    "        gamma=0.1,\n",
    "        milestones=[\n",
    "            170,\n",
    "            200,\n",
    "        ],\n",
    "        type='MultiStepLR'),\n",
    "]\n",
    "resume = False\n",
    "test_cfg = dict()\n",
    "test_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    dataset=dict(\n",
    "        ann_file='annotations/mpii_val.json',\n",
    "        data_mode='topdown',\n",
    "        data_prefix=dict(img='images/'),\n",
    "        data_root='data/mpii/',\n",
    "        headbox_file='data/mpii/annotations/mpii_gt_val.mat',\n",
    "        pipeline=[\n",
    "            dict(type='LoadImage'),\n",
    "            dict(type='GetBBoxCenterScale'),\n",
    "            dict(input_size=(\n",
    "                256,\n",
    "                256,\n",
    "            ), type='TopdownAffine'),\n",
    "            dict(type='PackPoseInputs'),\n",
    "        ],\n",
    "        test_mode=True,\n",
    "        type='MpiiDataset'),\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(round_up=False, shuffle=False, type='DefaultSampler'))\n",
    "test_evaluator = dict(type='MpiiPCKAccuracy')\n",
    "train_cfg = dict(by_epoch=True, max_epochs=210, val_interval=10)\n",
    "train_dataloader = dict(\n",
    "    batch_size=64,\n",
    "    dataset=dict(\n",
    "        ann_file='annotations/mpii_train.json',\n",
    "        data_mode='topdown',\n",
    "        data_prefix=dict(img='images/'),\n",
    "        data_root='data/mpii/',\n",
    "        pipeline=[\n",
    "            dict(type='LoadImage'),\n",
    "            dict(type='GetBBoxCenterScale'),\n",
    "            dict(direction='horizontal', type='RandomFlip'),\n",
    "            dict(shift_prob=0, type='RandomBBoxTransform'),\n",
    "            dict(input_size=(\n",
    "                256,\n",
    "                256,\n",
    "            ), type='TopdownAffine'),\n",
    "            dict(\n",
    "                encoder=dict(\n",
    "                    heatmap_size=(\n",
    "                        64,\n",
    "                        64,\n",
    "                    ),\n",
    "                    input_size=(\n",
    "                        256,\n",
    "                        256,\n",
    "                    ),\n",
    "                    sigma=2,\n",
    "                    type='MSRAHeatmap',\n",
    "                    unbiased=True),\n",
    "                type='GenerateTarget'),\n",
    "            dict(type='PackPoseInputs'),\n",
    "        ],\n",
    "        type='MpiiDataset'),\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImage'),\n",
    "    dict(type='GetBBoxCenterScale'),\n",
    "    dict(direction='horizontal', type='RandomFlip'),\n",
    "    dict(shift_prob=0, type='RandomBBoxTransform'),\n",
    "    dict(input_size=(\n",
    "        256,\n",
    "        256,\n",
    "    ), type='TopdownAffine'),\n",
    "    dict(\n",
    "        encoder=dict(\n",
    "            heatmap_size=(\n",
    "                64,\n",
    "                64,\n",
    "            ),\n",
    "            input_size=(\n",
    "                256,\n",
    "                256,\n",
    "            ),\n",
    "            sigma=2,\n",
    "            type='MSRAHeatmap',\n",
    "            unbiased=True),\n",
    "        type='GenerateTarget'),\n",
    "    dict(type='PackPoseInputs'),\n",
    "]\n",
    "val_cfg = dict()\n",
    "val_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    dataset=dict(\n",
    "        ann_file='annotations/mpii_val.json',\n",
    "        data_mode='topdown',\n",
    "        data_prefix=dict(img='images/'),\n",
    "        data_root='data/mpii/',\n",
    "        headbox_file='data/mpii/annotations/mpii_gt_val.mat',\n",
    "        pipeline=[\n",
    "            dict(type='LoadImage'),\n",
    "            dict(type='GetBBoxCenterScale'),\n",
    "            dict(input_size=(\n",
    "                256,\n",
    "                256,\n",
    "            ), type='TopdownAffine'),\n",
    "            dict(type='PackPoseInputs'),\n",
    "        ],\n",
    "        test_mode=True,\n",
    "        type='MpiiDataset'),\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(round_up=False, shuffle=False, type='DefaultSampler'))\n",
    "val_evaluator = dict(type='MpiiPCKAccuracy')\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImage'),\n",
    "    dict(type='GetBBoxCenterScale'),\n",
    "    dict(input_size=(\n",
    "        256,\n",
    "        256,\n",
    "    ), type='TopdownAffine'),\n",
    "    dict(type='PackPoseInputs'),\n",
    "]\n",
    "vis_backends = [\n",
    "    dict(type='LocalVisBackend'),\n",
    "]\n",
    "visualizer = dict(\n",
    "    name='visualizer',\n",
    "    type='PoseLocalVisualizer',\n",
    "    vis_backends=[\n",
    "        dict(type='LocalVisBackend'),\n",
    "    ])\n"
   ],
   "metadata": {
    "id": "q6crRjRdsYDw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python /content/mmpose/tools/test.py \\\n",
    "        /content/models/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256-custom.py \\\n",
    "        /content/models/hrnet_w48_mpii_256x256_dark-0decd39f_20200927.pth"
   ],
   "metadata": {
    "id": "DH94rr_bsY0Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pose detection on some image"
   ],
   "metadata": {
    "id": "XgzgaC0w3ZzU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmcv import imread\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples\n",
    "\n",
    "\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "\n",
    "local_runtime = False\n",
    "\n",
    "try:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "except:\n",
    "    local_runtime = True"
   ],
   "metadata": {
    "id": "aiVyaRRLuDqH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "joints = {\n",
    "    (5, 4),\n",
    "    (4, 3),\n",
    "    (0, 1),\n",
    "    (1, 2),\n",
    "    (3, 2),\n",
    "    (3, 6),\n",
    "    (2, 6),\n",
    "    (6, 7),\n",
    "    (7, 8),\n",
    "    (8, 9),\n",
    "    (13, 7),\n",
    "    (12, 7),\n",
    "    (13, 14),\n",
    "    (12, 11),\n",
    "    (14, 15),\n",
    "    (11, 10),\n",
    "}"
   ],
   "metadata": {
    "id": "S-LU6kDLIt56"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "img1 = \"/content/data/mpii/images/033441445.jpg\"\n",
    "img2 = \"/content/data/mpii/images/000061164.jpg\""
   ],
   "metadata": {
    "id": "x6ZgMHFduDoE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pose_config = \"/content/models/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256-custom.py\"\n",
    "pose_checkpoint = \"/content/models/hrnet_w48_mpii_256x256_dark-0decd39f_20200927.pth\"\n",
    "det_config = \"/content/mmpose/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py\"\n",
    "det_checkpoint = \"https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\""
   ],
   "metadata": {
    "id": "csOvdF4PuDld"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg_options = dict(model=dict(test_cfg=dict(output_heatmaps=True)))"
   ],
   "metadata": {
    "id": "1quzR7e1uDi2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# build detector\n",
    "detector = init_detector(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# build pose estimator\n",
    "pose_estimator = init_pose_estimator(\n",
    "    pose_config,\n",
    "    pose_checkpoint,\n",
    "    device=device,\n",
    "    cfg_options=cfg_options,\n",
    ")"
   ],
   "metadata": {
    "id": "jFHTJFe79RyE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# init visualizer\n",
    "pose_estimator.cfg.visualizer.radius = 3\n",
    "pose_estimator.cfg.visualizer.line_width = 1\n",
    "pose_estimator.test_cfg[\"flip_test\"] = False\n",
    "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
    "# the dataset_meta is loaded from the checkpoint and\n",
    "# then pass to the model in init_pose_estimator\n",
    "visualizer.set_dataset_meta(pose_estimator.dataset_meta)"
   ],
   "metadata": {
    "id": "AzKWIAdw3-UU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_img(img_path, detector, pose_estimator, visualizer, show_interval, out_file):\n",
    "    \"\"\"Visualize predicted keypoints (and heatmaps) of one image.\"\"\"\n",
    "    # predict bbox\n",
    "    scope = detector.cfg.get(\"default_scope\", \"mmdet\")\n",
    "    if scope is not None:\n",
    "        init_default_scope(scope)\n",
    "    detect_result = inference_detector(detector, img_path)\n",
    "    pred_instance = detect_result.pred_instances.cpu().numpy()\n",
    "    bboxes = np.concatenate((pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0, pred_instance.scores > 0.3)]\n",
    "    bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
    "\n",
    "    # predict keypoints\n",
    "    pose_results = inference_topdown(pose_estimator, img_path, bboxes)\n",
    "    data_samples = merge_data_samples(pose_results)\n",
    "\n",
    "    # show the results\n",
    "    img = mmcv.imread(img_path, channel_order=\"rgb\")\n",
    "\n",
    "    visualizer.add_datasample(\n",
    "        \"result\",\n",
    "        img,\n",
    "        data_sample=data_samples,\n",
    "        draw_gt=False,\n",
    "        draw_heatmap=False,\n",
    "        draw_bbox=False,\n",
    "        show=False,\n",
    "        wait_time=show_interval,\n",
    "        out_file=out_file,\n",
    "        kpt_thr=0.3,\n",
    "    )"
   ],
   "metadata": {
    "id": "VT_MKvGZ4fZ-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "visualize_img(img1, detector, pose_estimator, visualizer, show_interval=0, out_file=None)\n",
    "\n",
    "vis_result = visualizer.get_image()"
   ],
   "metadata": {
    "id": "1BTcwmzC4fXY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if local_runtime:\n",
    "    import os.path as osp\n",
    "    import tempfile\n",
    "\n",
    "    import cv2\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file_name = osp.join(tmpdir, \"pose_results.png\")\n",
    "        cv2.imwrite(file_name, vis_result[:, :, ::-1])\n",
    "        display(Image(file_name))\n",
    "else:\n",
    "    cv2_imshow(vis_result[:, :, ::-1])  # RGB2BGR to fit cv2"
   ],
   "metadata": {
    "id": "huLknmHI4jL9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def visualize_keypoints(img, image_path, keypoints, joints):\n",
    "    if image_path is not None:\n",
    "        # Load the image\n",
    "        img = Image.open(image_path)\n",
    "        h, w = img.height, img.width\n",
    "    else:\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "    # Unpack the keypoints from the shape (1, 17, 2)\n",
    "    keypoints = keypoints[0]\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Plot keypoints\n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        ax.scatter(x, y, c=\"r\", marker=\"o\")\n",
    "        ax.text(x, y, f\"{i}\", color=\"r\", fontsize=8)\n",
    "\n",
    "    # Plot joints\n",
    "    for i, j in joints:\n",
    "        x_i, y_i = keypoints[i]\n",
    "        x_j, y_j = keypoints[j]\n",
    "        line = mlines.Line2D([x_i, x_j], [y_i, y_j], color=\"g\")\n",
    "        ax.add_line(line)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(0, w)\n",
    "    ax.set_ylim(h, 0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "O9DQVHkVUHrt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## measure metrics between 2 images"
   ],
   "metadata": {
    "id": "-A4RtjWW464E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ],
   "metadata": {
    "id": "Yn5E_njNBzRs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def get_keypoints(img_path, detector, pose_estimator):\n",
    "    scope = detector.cfg.get(\"default_scope\", \"mmdet\")\n",
    "    if scope is not None:\n",
    "        init_default_scope(scope)\n",
    "    detect_result = inference_detector(detector, img_path)\n",
    "    pred_instance = detect_result.pred_instances.cpu().numpy()\n",
    "    bboxes = np.concatenate((pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0, pred_instance.scores > 0.3)]\n",
    "    bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
    "\n",
    "    # predict keypoints\n",
    "    pose_results = inference_topdown(pose_estimator, img_path, bboxes)[0].pred_instances\n",
    "    keypoints = np.concatenate(\n",
    "        (pose_results.keypoints, pose_results.keypoint_scores[..., None]),\n",
    "        axis=-1,\n",
    "    )\n",
    "    return keypoints"
   ],
   "metadata": {
    "id": "nCBOnbgI_Nii"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_vector(vector):\n",
    "    magnitude = np.linalg.norm(vector)\n",
    "    if magnitude == 0:\n",
    "        return vector  # Avoid division by zero\n",
    "    return vector / magnitude\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)\n",
    "\n",
    "\n",
    "def compute_cosine_similarity_for_joints(keypoints1, keypoints2, joints):\n",
    "    similarities = []\n",
    "\n",
    "    for i, j in joints:\n",
    "        vector1 = keypoints1[j] - keypoints1[i]\n",
    "        vector2 = keypoints2[j] - keypoints2[i]\n",
    "\n",
    "        normalized_vector1 = normalize_vector(vector1)\n",
    "        normalized_vector2 = normalize_vector(vector2)\n",
    "\n",
    "        cosine_similarity = compute_cosine_similarity(normalized_vector1, normalized_vector2)\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    average_cos_sililarity = sum(similarities) / len(similarities)\n",
    "\n",
    "    return similarities, average_cos_sililarity, np.min(np.abs(similarities))\n",
    "\n",
    "\n",
    "def compute_cosine_similarity_for_joints_with_weights(keypoints1, keypoints2, weights, joints):\n",
    "    similarities = []\n",
    "\n",
    "    for index, (i, j) in enumerate(joints):\n",
    "        vector1 = keypoints1[j] - keypoints1[i]\n",
    "        vector2 = keypoints2[j] - keypoints2[i]\n",
    "\n",
    "        normalized_vector1 = normalize_vector(vector1)\n",
    "        normalized_vector2 = normalize_vector(vector2)\n",
    "        # print(compute_cosine_similarity(normalized_vector1, normalized_vector2))\n",
    "        cosine_similarity = weights[index] * compute_cosine_similarity(normalized_vector1, normalized_vector2)\n",
    "\n",
    "        similarities.append(cosine_similarity)\n",
    "\n",
    "    # average_cos_sililarity = sum(similarities)/ len(similarities)\n",
    "    average_cos_sililarity = sum(similarities) / sum(weights)\n",
    "\n",
    "    return similarities, average_cos_sililarity, np.min(np.abs(similarities))"
   ],
   "metadata": {
    "id": "TTijtEbSB-9H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def compute_oks(prediction_poses, ground_truth_poses, sigma=10):\n",
    "    \"\"\"Compute average Object Keypoint Similarity (OKS) between predicted and ground truth poses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - prediction_poses: List of predicted poses (each pose is an Nx3 array where N is the number of keypoints)\n",
    "    - ground_truth_poses: List of ground truth poses (each pose is an Nx3 array)\n",
    "    - sigma: Standard deviation for the Gaussian smoothing (default is 0.25)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - Average OKS score across all frames\n",
    "    \"\"\"\n",
    "    total_oks = 0\n",
    "    num_frames = min(len(prediction_poses), len(ground_truth_poses))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        predicted_pose = normalize_vector(prediction_poses[i])\n",
    "        ground_truth_pose = normalize_vector(ground_truth_poses[i])\n",
    "\n",
    "        # Check if the poses are valid\n",
    "        if len(predicted_pose) == 0 or len(ground_truth_pose) == 0:\n",
    "            continue\n",
    "\n",
    "        d = cdist(predicted_pose[:, :2], ground_truth_pose[:, :2], \"euclidean\")\n",
    "        sigmas = sigma * np.maximum(ground_truth_pose[:, 2], 1e-14)\n",
    "        e = np.exp(-0.5 * (d / sigmas[:, None]) ** 2)\n",
    "        oks = np.sum(e) / len(e)\n",
    "\n",
    "        total_oks += oks\n",
    "\n",
    "    # Calculate the average OKS across all frames\n",
    "    if num_frames > 0:\n",
    "        average_oks = total_oks / num_frames\n",
    "    else:\n",
    "        average_oks = 0.0\n",
    "\n",
    "    return average_oks"
   ],
   "metadata": {
    "id": "c0II8sz1cpaJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_keypoints(img1, detector, pose_estimator).shape"
   ],
   "metadata": {
    "id": "6vX9kC2r46Tm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keypoints_with_scores_1 = get_keypoints(img1, detector, pose_estimator)\n",
    "keypoints_with_scores_2 = get_keypoints(img2, detector, pose_estimator)"
   ],
   "metadata": {
    "id": "yVKx1a7iB8d9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "keypoints_with_scores_1.squeeze().shape"
   ],
   "metadata": {
    "id": "mTxJkmjTOZx8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_oks(\n",
    "    keypoints_with_scores_1,\n",
    "    keypoints_with_scores_2,\n",
    ")"
   ],
   "metadata": {
    "id": "H9zRgrZodOz6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_cosine_similarity_for_joints(\n",
    "    keypoints_with_scores_1[:, :, 0:2].squeeze(), keypoints_with_scores_2[:, :, 0:2].squeeze(), joints\n",
    ")"
   ],
   "metadata": {
    "id": "ZJRAD9179SMg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_cosine_similarity_for_joints_with_weights(\n",
    "    keypoints_with_scores_1[:, :, 0:2].squeeze(),\n",
    "    keypoints_with_scores_2[:, :, 0:2].squeeze(),\n",
    "    keypoints_with_scores_1[:, :, 2].squeeze(),\n",
    "    joints,\n",
    ")"
   ],
   "metadata": {
    "id": "JgK-xq94HHam"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## measure metrics between video and image"
   ],
   "metadata": {
    "id": "siEfpsTbTuPp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from decord import VideoReader, cpu, gpu\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "amtnj1DONKBA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vr = VideoReader(\"/content/data/video.mp4\", ctx=cpu(0))\n",
    "print(\"video frames:\", len(vr))"
   ],
   "metadata": {
    "id": "WUkNf-2HAMgE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "target_frame_index = 470\n",
    "target_frame = vr[target_frame_index].asnumpy()\n",
    "keypoints_target = get_keypoints(target_frame, detector, pose_estimator)"
   ],
   "metadata": {
    "id": "T-AEfWTIT41u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "visualize_keypoints(vr[target_frame_index].asnumpy(), None, keypoints_target[:, :, 0:2], joints)"
   ],
   "metadata": {
    "id": "KYWPg11WT6Rk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "oks_distance = []\n",
    "cosine_joints_similarity_avg = []\n",
    "cosine_joints_similarity_min = []\n",
    "cosine_weight_joints_similarity_avg = []\n",
    "cosine_weight_joints_similarity_min = []\n",
    "\n",
    "w, h = vr[target_frame_index].shape[1], vr[target_frame_index].shape[0]\n",
    "output = cv2.VideoWriter(\"output.mp4\", cv2.VideoWriter_fourcc(*\"MP4V\"), vr.get_avg_fps(), (w * 2, h))  # *'DIVX'\n",
    "\n",
    "for i in tqdm(range(len(vr))):\n",
    "    # the video reader will handle seeking and skipping in the most efficient manner\n",
    "    frame = vr[i].asnumpy()\n",
    "    keypoints_frame = get_keypoints(frame, detector, pose_estimator)\n",
    "\n",
    "    oks_distance.append(\n",
    "        compute_oks(\n",
    "            keypoints_frame,\n",
    "            keypoints_target,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    _, dist_avg, dist_min = compute_cosine_similarity_for_joints(\n",
    "        keypoints_frame[:, :, 0:2].squeeze(), keypoints_target[:, :, 0:2].squeeze(), joints\n",
    "    )\n",
    "\n",
    "    _, dist_w_avg, dist_w_min = compute_cosine_similarity_for_joints_with_weights(\n",
    "        keypoints_frame[:, :, 0:2].squeeze(),\n",
    "        keypoints_target[:, :, 0:2].squeeze(),\n",
    "        keypoints_frame[:, :, 2].squeeze(),\n",
    "        joints,\n",
    "    )\n",
    "\n",
    "    cosine_joints_similarity_avg.append(dist_avg)\n",
    "    cosine_joints_similarity_min.append(dist_min)\n",
    "    cosine_weight_joints_similarity_avg.append(dist_w_avg)\n",
    "    cosine_weight_joints_similarity_min.append(dist_w_min)\n",
    "\n",
    "    out_frame = cv2.hconcat([frame, target_frame])\n",
    "\n",
    "    # get boundary of this text\n",
    "    textsize = cv2.getTextSize(\"OKS: 0.0000\", cv2.FONT_HERSHEY_DUPLEX, 1, 2)[0]\n",
    "\n",
    "    # get coords based on boundary\n",
    "    textX = (out_frame.shape[1] - textsize[0]) // 2\n",
    "    textY = textsize[1]\n",
    "\n",
    "    cv2.putText(\n",
    "        out_frame,\n",
    "        text=f\"OKS: {round(oks_distance[-1], 4)}\",\n",
    "        org=(textX, textY),\n",
    "        fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "        fontScale=1.0,\n",
    "        color=(0, 0, 0),\n",
    "        thickness=2,\n",
    "    )\n",
    "    output.write(out_frame)\n",
    "\n",
    "output.release()"
   ],
   "metadata": {
    "id": "JzNW7o67NrE5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(oks_distance, label=\"oks_distance\")\n",
    "plt.plot(cosine_joints_similarity_avg, label=\"cosine_joints_similarity_avg\")\n",
    "# plt.plot(cosine_joints_similarity_min, label = \"cosine_joints_similarity_min\")\n",
    "plt.plot(cosine_weight_joints_similarity_avg, label=\"cosine_weight_joints_similarity_avg\")\n",
    "# plt.plot(cosine_weight_joints_similarity_min, label = \"cosine_weight_joints_similarity_min\")\n",
    "\n",
    "plt.axvline(x=target_frame_index, color=\"red\", label=\"target frame\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "8POdfhbqShY6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## add metrics to video"
   ],
   "metadata": {
    "id": "ib335UoGTlVU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from enum import Enum\n",
    "# from typing import Callable\n",
    "\n",
    "# try:\n",
    "#     import cv2\n",
    "# except ImportError:\n",
    "#     raise Exception(\"Looks like you don't have cv2 installed. \"\n",
    "#                     \"Please check https://pypi.org/project/opencv-python/ on how to install it.\"\n",
    "#                     \"Or: pip install opencv-python\")\n",
    "\n",
    "# from time import time, sleep\n",
    "\n",
    "\n",
    "# class FrameChange(Enum):\n",
    "#     NoChange = 0\n",
    "#     Next = 1\n",
    "#     Seek = 2\n",
    "\n",
    "\n",
    "# class CV2VideoPlayer:\n",
    "#     KEY_CODE_STOP = 27\n",
    "#     KEY_CODE_TOGGLE_PLAY = 32\n",
    "#     KEY_CODE_PREV_FRAME = ord('a')\n",
    "#     KEY_CODE_NEXT_FRAME = ord('d')\n",
    "\n",
    "#     CMD_NEXT_FRAME = \"next\"\n",
    "#     CMD_NOOP = \"noop\"\n",
    "\n",
    "#     # __cap:\n",
    "#     __frame_count: int\n",
    "#     __fps: int\n",
    "#     __on_frame_callback: Callable\n",
    "#     __on_stop_callback: Callable\n",
    "#     __previous_frame_display_timestamp: float\n",
    "#     __window_name: str = 'VideoPlayer'\n",
    "\n",
    "#     def __init__(self, filename: str, on_frame: Callable, on_stop: Callable):\n",
    "#         self.__cap = cv2.VideoCapture(filename)\n",
    "#         if not self.__cap.isOpened():\n",
    "#             raise Exception(f\"Could not read {filename}\")\n",
    "\n",
    "#         self.__frame_count = int(self.__cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         self.__fps = self.__cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#         self.__playback_rate = 1.0\n",
    "\n",
    "#         self.__current_frame = 0\n",
    "#         self.__on_frame_callback = on_frame\n",
    "#         self.__on_stop_callback = on_stop\n",
    "#         self.__previous_frame_display_timestamp = 0\n",
    "\n",
    "#         self.__status = 'play'\n",
    "\n",
    "#         self.__setup_ui()\n",
    "\n",
    "#     def __setup_ui(self):\n",
    "#         def on_change_frame(x):\n",
    "#             if x == self.__current_frame:\n",
    "#                 return\n",
    "\n",
    "#             self.__current_frame = x\n",
    "#             if self.__status == 'paused':\n",
    "#                 self.__status = 'seek_frame'\n",
    "\n",
    "#         def on_change_playback_rate(x):\n",
    "#             if x == 0:\n",
    "#                 x = 1\n",
    "#             self.__playback_rate = x / 100.\n",
    "\n",
    "#         cv2.namedWindow(self.__window_name)\n",
    "#         cv2.createTrackbar('Frame', self.__window_name, 0, self.__frame_count - 1, on_change_frame)\n",
    "#         cv2.setTrackbarPos('Frame', self.__window_name, 0)\n",
    "\n",
    "#         cv2.createTrackbar('Playback Speed', self.__window_name, 1, 400, on_change_playback_rate)\n",
    "#         cv2.setTrackbarPos('Playback Speed', self.__window_name, int(self.__playback_rate * 100))\n",
    "\n",
    "#     def __handle_keyboard_input(self):\n",
    "#         key = cv2.waitKey(1)\n",
    "#         if key == self.KEY_CODE_TOGGLE_PLAY:\n",
    "#             if self.__status == 'paused':\n",
    "#                 self.__status = 'play'\n",
    "#             else:\n",
    "#                 self.__status = 'paused'\n",
    "#         elif key == self.KEY_CODE_PREV_FRAME:\n",
    "#             self.__status = 'prev_frame'\n",
    "#         elif key == self.KEY_CODE_NEXT_FRAME:\n",
    "#             self.__status = 'next_frame'\n",
    "#         elif key == self.KEY_CODE_STOP:\n",
    "#             self.stop()\n",
    "\n",
    "#     def __calculate_current_frame(self) -> FrameChange:\n",
    "#         if self.__status == 'play':\n",
    "#             if self.__current_frame == self.__frame_count:\n",
    "#                 self.__status = 'paused'\n",
    "#                 return FrameChange.NoChange\n",
    "\n",
    "#             now = time()\n",
    "#             if self.__previous_frame_display_timestamp == 0:\n",
    "#                 self.__previous_frame_display_timestamp = now\n",
    "#                 return FrameChange.NoChange\n",
    "\n",
    "#             frame_display_time = 1 / self.__fps / self.__playback_rate\n",
    "#             time_delta = now - self.__previous_frame_display_timestamp\n",
    "\n",
    "#             frame_delta = int(time_delta / frame_display_time)\n",
    "\n",
    "#             if frame_delta > 0:\n",
    "#                 self.__current_frame += frame_delta\n",
    "#                 self.__previous_frame_display_timestamp = now\n",
    "#                 if frame_delta > 1:\n",
    "#                     return FrameChange.Seek\n",
    "#                 else:\n",
    "#                     return FrameChange.Next\n",
    "#         elif self.__status == 'next_frame':\n",
    "#             if self.__current_frame < self.__frame_count:\n",
    "#                 self.__current_frame += 1\n",
    "#                 self.__status = 'paused'\n",
    "#                 return FrameChange.Next\n",
    "#         elif self.__status == 'prev_frame':\n",
    "#             if self.__current_frame > 0:\n",
    "#                 self.__current_frame -= 1\n",
    "#                 self.__status = 'paused'\n",
    "#                 return FrameChange.Seek\n",
    "#         elif self.__status == 'seek_frame':\n",
    "#             self.__status = 'paused'\n",
    "#             return FrameChange.Seek\n",
    "#         elif self.__status == 'paused':\n",
    "#             self.__previous_frame_display_timestamp = 0\n",
    "\n",
    "#         return FrameChange.NoChange\n",
    "\n",
    "#     def __show_current_frame(self):\n",
    "#         ret, im = self.__cap.read()\n",
    "#         if im is None:\n",
    "#             return\n",
    "#         r = 720.0 / im.shape[1]\n",
    "#         dim = (720, int(im.shape[0] * r))\n",
    "#         im = cv2.resize(im, dim, interpolation=cv2.INTER_AREA)\n",
    "#         cv2.imshow(self.__window_name, im)\n",
    "\n",
    "#     def on_timer(self):\n",
    "#         if self.__status == \"stopped\":\n",
    "#             return\n",
    "\n",
    "#         self.__handle_keyboard_input()\n",
    "#         frame_change = self.__calculate_current_frame()\n",
    "\n",
    "#         if frame_change == FrameChange.NoChange:\n",
    "#             return\n",
    "#         elif frame_change == FrameChange.Next:\n",
    "#             pass  # by default capture device reads next frame\n",
    "#         elif frame_change == FrameChange.Seek:\n",
    "#             self.__cap.set(cv2.CAP_PROP_POS_FRAMES, self.__current_frame)\n",
    "\n",
    "#         self.__show_current_frame()\n",
    "#         cv2.setTrackbarPos('Frame', self.__window_name, self.__current_frame)\n",
    "#         self.__on_frame_callback(self.__current_frame / self.__fps)\n",
    "\n",
    "#     def stop(self):\n",
    "#         self.__cap.release()\n",
    "#         del self.__cap\n",
    "#         cv2.destroyWindow(self.__window_name)\n",
    "#         self.__status = \"stopped\"\n",
    "#         self.__on_stop_callback()\n",
    "\n",
    "\n",
    "# def attach_video_player_to_figure(figure, filename: str, on_frame: Callable, **callback_args):\n",
    "#     try:\n",
    "#         from matplotlib.figure import Figure\n",
    "#     except ImportError:\n",
    "#         raise Exception(\"Looks like you don't have matplotlib installed.\")\n",
    "\n",
    "#     if not isinstance(figure, Figure):\n",
    "#         raise ValueError(\"`figure` should be a matplotlib Figure instance\")\n",
    "\n",
    "#     timer = None\n",
    "\n",
    "#     def on_stop():\n",
    "#         timer.stop()\n",
    "\n",
    "#     video_player = CV2VideoPlayer(filename, lambda timestamp: on_frame(timestamp, **callback_args), on_stop)\n",
    "\n",
    "#     timer = figure.canvas.new_timer(interval=10,\n",
    "#                                     callbacks=[(video_player.on_timer, [], {})])\n",
    "#     timer.start()\n",
    "\n",
    "\n",
    "# def start_video_player(filename: str, on_frame: Callable):\n",
    "#     _should_stop = False\n",
    "\n",
    "#     def on_stop():\n",
    "#         nonlocal _should_stop\n",
    "#         _should_stop = True\n",
    "\n",
    "#     video_player = CV2VideoPlayer(filename, on_frame, on_stop)\n",
    "\n",
    "#     while not _should_stop:\n",
    "#         video_player.on_timer()\n",
    "#         sleep(0.01)"
   ],
   "metadata": {
    "id": "RsxgJ7HZktng"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = []\n",
    "\n",
    "\n",
    "# def on_frame(video_timestamp, line):\n",
    "#     x.append(video_timestamp)\n",
    "\n",
    "#     line.set_data(x, oks_distance[:len(x)])\n",
    "#     line.axes.relim()\n",
    "#     line.axes.autoscale_view()\n",
    "#     line.axes.figure.canvas.draw()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     fig, ax = plt.subplots()\n",
    "#     plt.xlim(-15, 15)\n",
    "#     plt.axvline(x=0, color='k', linestyle='--')\n",
    "\n",
    "#     line, = ax.plot([], [], color='blue')\n",
    "\n",
    "#     attach_video_player_to_figure(fig, \"/content/data/video.mp4\", on_frame, line=line)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# main()"
   ],
   "metadata": {
    "id": "ejtEA2bU4jEZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## visualize keypoints"
   ],
   "metadata": {
    "id": "Mm1IopDHTeoG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "visualize_keypoints(None, img1, keypoints_with_scores_1[:, :, 0:2], joints)"
   ],
   "metadata": {
    "id": "pm25sC79H6cw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "visualize_keypoints(None, img2, keypoints_with_scores_2[:, :, 0:2], joints)"
   ],
   "metadata": {
    "id": "ua5NsJ1yH6aZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eSbuXUeGH6Xk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "x2ce5HWWH6VC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tfiH-it2pSia"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ehhGFAiipSgA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "MA2no63JpSdb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ikapKkgEpSa2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "r4rt6gZjpSYe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VknWCuFqpSVn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "iPlwCQBmpSTR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Kf5_m44jpSQp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cJqvt6bmpSN2"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
